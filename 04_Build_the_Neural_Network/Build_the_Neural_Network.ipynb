{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3845b839-036a-49aa-8316-cca794ebbeec",
   "metadata": {},
   "source": [
    "# Build the Neural Network\n",
    "\n",
    "Neural networks comprise of layers/modules that perform operations on data. The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace provides all the building blocks you need to build your neural network. Every module in PyTorch subclasses the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily.\n",
    "\n",
    "In the following sections, we'll build a neural network to classify images in the FashionMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58a6fddf-e33a-443b-b0d8-6d39d705dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5053f-d8c8-434d-a26c-5ef8328ece5b",
   "metadata": {},
   "source": [
    "## Get Device for Training\n",
    "\n",
    "We want to be able to train our model on an [accelerator](https://pytorch.org/docs/stable/torch.html#accelerators) such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd09894a-987f-43ee-9c6a-8f5916cd3baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CPU-only version of PyTorch is installed -- no torch.accelerator module available.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    device = torch.accelerator.current_accelerator.type if torch.accelerator.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "except:\n",
    "    device = 'cpu'\n",
    "    print('The CPU-only version of PyTorch is installed -- no torch.accelerator module available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bbb042-a920-4168-ade3-3af7647748de",
   "metadata": {},
   "source": [
    "## Define the Class\n",
    "\n",
    "We define our neural network by subclassing `nn.Module` and initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations on input data in the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc6de8e-f2aa-4d3e-83fd-170fa1a8fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    '''\n",
    "    This is a 2-hidden-layer multilayer perceptron (MLP) with 512 units per\n",
    "    hidden layer and ReLU activations. It maps a 28x28 grayscale image to a\n",
    "    10-class classification output.\n",
    "    Forward pass flow:\n",
    "        [N,1,28,28] -> [N,784] -> [N,512] -> [N,512] -> [N,10]\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(  # ordered container, runs layers in sequence.\n",
    "            nn.Linear(28*28, 512),  # Layer 1: fully connected, projects flattened image -> 512 features\n",
    "            nn.ReLU(),              # Activation 1: applies ReLU element-wise (max(0,x)), introducing nonlinearities\n",
    "            nn.Linear(512, 512),    # Layer 2: hidden layer, keeps feature dimension 512\n",
    "            nn.ReLU(),              # Activation 2: applies ReLU element-wise (max(0,x)), introducing nonlinearities\n",
    "            nn.Linear(512, 10)      # Layer 3: output layer, 10 units = 1 unit per FashionMNIST class\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ff3ad-9f6a-46b2-b320-9171b56e75a2",
   "metadata": {},
   "source": [
    "We create an instance of `NeuralNetwork`, and move it to the `device`, and print its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f9d4b19-0b10-469d-8275-50390cd0ee6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aff67a-0bf7-47fb-837b-cadf8ef607be",
   "metadata": {},
   "source": [
    "To use the model, we pass it the input data. This executes the model's `forward`, along with some [background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866). Do not call `model.forward()` directly!\n",
    "\n",
    "Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of eah output. We get the prediction probabilities by passing it through an instance of `nn.Softmax` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48d60c12-dc8c-46ec-9905-4e530c613ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([8])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf7750-7045-4b8a-b3e5-1f4d21ce920b",
   "metadata": {},
   "source": [
    "## Model Layers\n",
    "\n",
    "Let's break down the layers in the fashionMNIST model. To illustrate it, we will take a sample minibatch of 3 images of size 28x28 and see what happens to it as we pass it through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00345319-78f5-4a6c-a697-f68fb8856183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3803446e-02c4-41d5-a208-88eee84b4a91",
   "metadata": {},
   "source": [
    "### `nn.Flatten`\n",
    "\n",
    "We initialize the [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (the minibatch dimension (at dim=0) is maintained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d179b42-cdbf-4309-b4c9-da26c4e8ebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67d121a-d8fd-41ea-96c7-629695db2074",
   "metadata": {},
   "source": [
    "### `nn.Linear`\n",
    "\n",
    "The [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) is a module that applies a linear transformation on the input using its stored weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e60ea793-cd18-4563-a84d-5820a340e397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48ea79d-fdb1-47a1-8d13-26d8857eaa9d",
   "metadata": {},
   "source": [
    "### `nn.ReLU`\n",
    "\n",
    "Non-linear activations are what create the complex mappings between the model's inputs and outputs. They are applies after linear transformations to introduce *nonlinearity*, helping neural networks to approximate more complex functions.\n",
    "\n",
    "In this model, we use [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) between our linear layers, but there's other activations to introduce non-linearity in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0c9e148-8676-49a7-9f0e-e16e7f13ed2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.4801, -0.3691,  0.4417, -0.0261,  0.4218,  0.0454, -0.0938, -0.0899,\n",
      "          0.6204,  0.3612,  0.2228,  0.9825, -0.3221, -0.2441,  0.5065,  0.0515,\n",
      "         -0.0164,  0.3303, -0.1608, -0.2209],\n",
      "        [ 0.3995, -0.2207,  0.1956, -0.0415,  0.0428, -0.0914, -0.3814, -0.1659,\n",
      "          0.3649,  0.1680,  0.3953,  0.4469, -0.4674, -0.3793,  0.1014,  0.6099,\n",
      "          0.2419, -0.1294, -0.1382, -0.3718],\n",
      "        [ 0.4446, -0.1656,  0.6187,  0.2144,  0.3120, -0.3731, -0.1368, -0.2520,\n",
      "          0.6985,  0.2370,  0.4992,  0.4284, -0.1564, -0.1241,  0.3514,  0.4877,\n",
      "          0.0074,  0.1786, -0.4963, -0.7153]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.4801, 0.0000, 0.4417, 0.0000, 0.4218, 0.0454, 0.0000, 0.0000, 0.6204,\n",
      "         0.3612, 0.2228, 0.9825, 0.0000, 0.0000, 0.5065, 0.0515, 0.0000, 0.3303,\n",
      "         0.0000, 0.0000],\n",
      "        [0.3995, 0.0000, 0.1956, 0.0000, 0.0428, 0.0000, 0.0000, 0.0000, 0.3649,\n",
      "         0.1680, 0.3953, 0.4469, 0.0000, 0.0000, 0.1014, 0.6099, 0.2419, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.4446, 0.0000, 0.6187, 0.2144, 0.3120, 0.0000, 0.0000, 0.0000, 0.6985,\n",
      "         0.2370, 0.4992, 0.4284, 0.0000, 0.0000, 0.3514, 0.4877, 0.0074, 0.1786,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7878c71-df33-4308-9773-bf407de757cb",
   "metadata": {},
   "source": [
    "### `nn.Sequential`\n",
    "\n",
    "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like `seq_modules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c36b6cc-50d2-4352-812d-f4616c8e939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b919f-4c08-4a2d-9fac-1e4e157d4476",
   "metadata": {},
   "source": [
    "### `nn.Softmax`\n",
    "\n",
    "The last linear layer of the neural network returns *logits* - raw values in \\[-infty, infty\\] - which are passed to the [nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) module. The logits are scaled to values \\[0, 1\\] representing the model's predicted probabilities for each class. `dim` parameter indicated the dimension along which the values must sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ade10f83-da72-4f9f-b11b-6e266231775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e2d89b-4647-483b-903c-a988d9d1fc5f",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "\n",
    "Many layers inside a neural network are *parameterized*, i.e., have associated weights and biases that are optimized during training. Subclassing `nn.Module` automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model's `parameters()` or `named_parameters` methods.\n",
    "\n",
    "In this example, we iterate over each parameter, and print its size and a preview of its values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73357cf1-a9e2-473f-8766-aebe04d9d9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0340, -0.0118, -0.0042,  ...,  0.0048, -0.0260, -0.0048],\n",
      "        [-0.0175,  0.0044, -0.0259,  ...,  0.0286,  0.0203, -0.0347]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0322,  0.0206], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0127, -0.0041, -0.0177,  ..., -0.0060, -0.0110,  0.0129],\n",
      "        [ 0.0323,  0.0264,  0.0414,  ...,  0.0208, -0.0281, -0.0021]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0066, -0.0021], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0104, -0.0107, -0.0351,  ..., -0.0295, -0.0319,  0.0056],\n",
      "        [-0.0220,  0.0274, -0.0020,  ..., -0.0173,  0.0125,  0.0112]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0181,  0.0246], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
