{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3845b839-036a-49aa-8316-cca794ebbeec",
   "metadata": {},
   "source": [
    "# Build the Neural Network\n",
    "\n",
    "Neural networks comprise of layers/modules that perform operations on data. The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace provides all the building blocks you need to build your neural network. Every module in PyTorch subclasses the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily.\n",
    "\n",
    "In the following sections, we'll build a neural network to classify images in the FashionMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58a6fddf-e33a-443b-b0d8-6d39d705dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "# from torch import nn\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5053f-d8c8-434d-a26c-5ef8328ece5b",
   "metadata": {},
   "source": [
    "## Get Device for Training\n",
    "\n",
    "We want to be able to train our model on an [accelerator](https://pytorch.org/docs/stable/torch.html#accelerators) such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd09894a-987f-43ee-9c6a-8f5916cd3baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CPU-only version of PyTorch is installed -- no torch.accelerator module available.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    device = torch.accelerator.current_accelerator.type if torch.accelerator.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "except:\n",
    "    device = 'cpu'\n",
    "    print('The CPU-only version of PyTorch is installed -- no torch.accelerator module available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bbb042-a920-4168-ade3-3af7647748de",
   "metadata": {},
   "source": [
    "## Define the Class\n",
    "\n",
    "We define our neural network by subclassing `nn.Module` and initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations on input data in the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc6de8e-f2aa-4d3e-83fd-170fa1a8fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    '''\n",
    "    This is a 2-hidden-layer multilayer perceptron (MLP) with 512 units per\n",
    "    hidden layer and ReLU activations. It maps a 28x28 grayscale image to a\n",
    "    10-class classification output. Note: 28**2 = 784\n",
    "    Forward pass flow:\n",
    "        [N,1,28,28] -> [N,784] -> [N,512] -> [N,512] -> [N,10]\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear_relu_stack = torch.nn.Sequential(  # ordered container, runs layers in sequence.\n",
    "            torch.nn.Linear(28*28, 512),  # Layer 1: fully connected, projects flattened image -> 512 features\n",
    "            torch.nn.ReLU(),              # Activation 1: applies ReLU element-wise (max(0,x)), introducing nonlinearities\n",
    "            torch.nn.Linear(512, 512),    # Layer 2: hidden layer, keeps feature dimension 512\n",
    "            torch.nn.ReLU(),              # Activation 2: applies ReLU element-wise (max(0,x)), introducing nonlinearities\n",
    "            torch.nn.Linear(512, 10)      # Layer 3: output layer, 10 units = 1 unit per FashionMNIST class\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ff3ad-9f6a-46b2-b320-9171b56e75a2",
   "metadata": {},
   "source": [
    "We create an instance of `NeuralNetwork`, and move it to the `device`, and print its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f9d4b19-0b10-469d-8275-50390cd0ee6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aff67a-0bf7-47fb-837b-cadf8ef607be",
   "metadata": {},
   "source": [
    "To use the model, we pass it the input data. This executes the model's `forward`, along with some [background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866). Do not call `model.forward()` directly!\n",
    "\n",
    "Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of eah output. We get the prediction probabilities by passing it through an instance of `nn.Softmax` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48d60c12-dc8c-46ec-9905-4e530c613ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([5])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = torch.nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf7750-7045-4b8a-b3e5-1f4d21ce920b",
   "metadata": {},
   "source": [
    "## Model Layers\n",
    "\n",
    "Let's break down the layers in the fashionMNIST model. To illustrate it, we will take a sample minibatch of 3 images of size 28x28 and see what happens to it as we pass it through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00345319-78f5-4a6c-a697-f68fb8856183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3803446e-02c4-41d5-a208-88eee84b4a91",
   "metadata": {},
   "source": [
    "### `nn.Flatten`\n",
    "\n",
    "We initialize the [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (the minibatch dimension (at dim=0) is maintained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d179b42-cdbf-4309-b4c9-da26c4e8ebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = torch.nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67d121a-d8fd-41ea-96c7-629695db2074",
   "metadata": {},
   "source": [
    "### `nn.Linear`\n",
    "\n",
    "The [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) is a module that applies a linear transformation on the input using its stored weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e60ea793-cd18-4563-a84d-5820a340e397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = torch.nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48ea79d-fdb1-47a1-8d13-26d8857eaa9d",
   "metadata": {},
   "source": [
    "### `nn.ReLU`\n",
    "\n",
    "Non-linear activations are what create the complex mappings between the model's inputs and outputs. They are applies after linear transformations to introduce *nonlinearity*, helping neural networks to approximate more complex functions.\n",
    "\n",
    "In this model, we use [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) between our linear layers, but there's other activations to introduce non-linearity in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0c9e148-8676-49a7-9f0e-e16e7f13ed2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.3219,  0.1632,  0.0229,  0.0318,  0.2201,  0.2375,  0.2808, -0.0542,\n",
      "          0.2431,  0.5279,  0.1065,  0.0989,  0.0671, -0.2326, -0.0256,  0.4565,\n",
      "          0.1842, -0.0632, -0.9052,  0.4593],\n",
      "        [ 0.2207, -0.1200,  0.2723,  0.1421,  0.4299,  0.1876,  0.5424, -0.4376,\n",
      "          0.3253,  0.1706, -0.0618,  0.2151,  0.0845, -0.4501,  0.2252,  0.2839,\n",
      "          0.1492,  0.1834, -0.8131,  0.3164],\n",
      "        [ 0.1355,  0.0902,  0.1765,  0.5433,  0.6909,  0.3342,  0.3858, -0.0918,\n",
      "          0.4885,  0.1792,  0.1420,  0.8687,  0.0061, -0.3581, -0.1100, -0.1651,\n",
      "          0.1445,  0.2705, -0.5637,  0.6648]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.3219, 0.1632, 0.0229, 0.0318, 0.2201, 0.2375, 0.2808, 0.0000, 0.2431,\n",
      "         0.5279, 0.1065, 0.0989, 0.0671, 0.0000, 0.0000, 0.4565, 0.1842, 0.0000,\n",
      "         0.0000, 0.4593],\n",
      "        [0.2207, 0.0000, 0.2723, 0.1421, 0.4299, 0.1876, 0.5424, 0.0000, 0.3253,\n",
      "         0.1706, 0.0000, 0.2151, 0.0845, 0.0000, 0.2252, 0.2839, 0.1492, 0.1834,\n",
      "         0.0000, 0.3164],\n",
      "        [0.1355, 0.0902, 0.1765, 0.5433, 0.6909, 0.3342, 0.3858, 0.0000, 0.4885,\n",
      "         0.1792, 0.1420, 0.8687, 0.0061, 0.0000, 0.0000, 0.0000, 0.1445, 0.2705,\n",
      "         0.0000, 0.6648]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = torch.nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7878c71-df33-4308-9773-bf407de757cb",
   "metadata": {},
   "source": [
    "### `nn.Sequential`\n",
    "\n",
    "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like `seq_modules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c36b6cc-50d2-4352-812d-f4616c8e939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = torch.nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b919f-4c08-4a2d-9fac-1e4e157d4476",
   "metadata": {},
   "source": [
    "### `nn.Softmax`\n",
    "\n",
    "The last linear layer of the neural network returns *logits* - raw values in \\[-infty, infty\\] - which are passed to the [nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) module. The logits are scaled to values \\[0, 1\\] representing the model's predicted probabilities for each class. `dim` parameter indicated the dimension along which the values must sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ade10f83-da72-4f9f-b11b-6e266231775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e2d89b-4647-483b-903c-a988d9d1fc5f",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "\n",
    "Many layers inside a neural network are *parameterized*, i.e., have associated weights and biases that are optimized during training. Subclassing `nn.Module` automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model's `parameters()` or `named_parameters` methods.\n",
    "\n",
    "In this example, we iterate over each parameter, and print its size and a preview of its values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73357cf1-a9e2-473f-8766-aebe04d9d9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0118, -0.0175, -0.0148,  ..., -0.0006,  0.0309, -0.0051],\n",
      "        [-0.0314, -0.0119,  0.0152,  ..., -0.0070, -0.0276, -0.0191]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0098, -0.0185], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0015,  0.0106, -0.0271,  ...,  0.0327,  0.0035,  0.0215],\n",
      "        [-0.0011,  0.0161, -0.0354,  ...,  0.0017, -0.0257,  0.0049]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0034, 0.0207], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0161, -0.0387,  0.0116,  ..., -0.0262,  0.0038,  0.0440],\n",
      "        [ 0.0401,  0.0030,  0.0102,  ...,  0.0310,  0.0136, -0.0009]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0359,  0.0353], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
